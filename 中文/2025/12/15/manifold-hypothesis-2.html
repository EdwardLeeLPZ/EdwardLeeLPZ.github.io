<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 流形假设(Manifold Hypothesis)下的思考·二 | Peizheng Li </title> <meta name="author" content="Peizheng Li"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?343676cba58161999546fb5ac8b9e53d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://edwardleelpz.github.io/%E4%B8%AD%E6%96%87/2025/12/15/manifold-hypothesis-2.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Peizheng</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">流形假设(Manifold Hypothesis)下的思考·二</h1> <p class="post-meta"> Created on December 15, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> Tech-Blog   ·   <i class="fa-solid fa-tag fa-sm"></i> 中文 </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="引言架构的演进与来时的路"><strong>引言：架构的演进与来时的路</strong></h2> <p>很长时间以来，我们一直在期待一个足已取代Transformer或者现有Attention机制的新一代架构。无论是前两年志在整体颠覆Transformer的Mamba系列还是近期被越来越多提及的Sparse/Gated Attention这样基于已有机制的迭代和优化，研究在不断前进，目标却仍忽远忽近。相比于难以捉摸的未来，也许来时的路会更加清晰一些。</p> <h2 id="一从mlp到transformer"><strong>一、从MLP到Transformer</strong></h2> <p>从流形假设的角度，所有的经典架构都可以被视为带有不同Inductive Bias的从卷曲流形到平坦特征空间的拓扑变换。</p> <h3 id="1-mlp无结构先验的通用流形算子"><strong>1. MLP：无结构先验的通用流形算子</strong></h3> <p>MLP 将输入空间 \(R^D\) 视为平坦的欧几里得空间，利用层级化的仿射变换和非线性激活函数，对整个输入空间进行无差别的折叠、扭曲和拉伸。 根据 Universal Approximation Theorem，只要有足够的神经元，MLP 可以逼近任何连续函数。这意味着它理论上可以将任何拓扑结构的流形变换为线性可分形态。</p> <ul> <li> <p><strong>最差的算子</strong>：MLP结构内部几乎没有内嵌任何Inductive Bias，其核心假设是“平滑性”，即相近的输入产生相近的输出。数学上，这要求学习的函数满足Lipschitz连续条件：\(∥f(x)-f(y)∥≤K∥x−y∥\) 。然而，在高维空间 \(R^D\) 中，若没有特定的几何先验，要保证这种平滑性，所需的样本数量 \(N\) 随维度 \(D\) 呈指数增长。此外，这种基于欧氏距离 \(∥x−y∥_2\) 的度量在高维空间中失效，所有点对之间的距离趋于一致。这种<strong>全空间各向同性的假设</strong>忽视了高维流形结构的复杂性，是其效率低下的根本原因。因此，MLP的大规模使用总是会导致最差的性能。</p> </li> <li> <p><strong>最好的算子</strong>：从另一个角度讲，几乎没有Inductive Bias意味着对于输入数据的分布没有任何要求，MLP具备对于所有流形的折叠与投影能力。同时作为刚性变换算子， MLP不改变数据的拓扑结构。因此，在跨模态特征融合（embedding的直接对齐）以及大多数downstream head（特征空间足够平坦）中，MLP成为了最优（最通用）的选择。这也造就了诸如LLaVA等大道至简的方法。</p> </li> </ul> <h3 id="2-cnn欧氏群对称性的引入"><strong>2. CNN：欧氏群对称性的引入</strong></h3> <p>CNN 的成功源于它显式地利用了自然图像流形的<strong>Geometric Priors</strong>：</p> <ul> <li> <strong>局部连接</strong>：它假设流形具有局部拓扑结构，像素的相关性随距离衰减。</li> <li> <strong>权值共享</strong>：它假设流形上各处的切空间几何性质是均匀的，流形 \(M\) 对平移群 \(SE(2)\) 是不变的，即： \(f(T(x))=f(x)\) （不变性）或 \(f(T(x))=T′(f(x))\) （等变性）。</li> </ul> <p>CNN作为空间局部算子，显式地在网络结构中硬编码了流形的<strong>对称性</strong>，在输入流形的局部邻域做“低通滤波”，削弱高频噪声与不稳定方向，从而让表示更接近低维结构。这可以近似理解为CNN在对整个流形同时及进行多个等间距的折叠和扭曲。</p> <p>但是卷积神经网络依然保留了欧氏几何度量，当数据流形具有非零曲率（如球面投影数据、非欧几何图数据）时，CNN的平移不变性假设失效，导致其无法有效捕捉长程依赖，因为在弯曲流形上，“直线”最短路径变成了测地线，而固定大小的卷积核无法覆盖测地线距离。</p> <h3 id="3-rnn状态流形上的动力系统轨迹"><strong>3. RNN：状态流形上的动力系统轨迹</strong></h3> <p>RNN将数据流形建模为一个随时间演化的Dynamical System：</p> \[h_t=σ(W_h h_{t−1} + W_x x_t)\] <p>这本质上是常微分方程 \(\frac{dh}{dt}=f(h,x)\) 的离散化欧拉积分。RNN 是一个时间递推算子，试图学习流形的切空间向量场。每一个隐状态 \(h_t\) 都是流形上的一个坐标点，权重矩阵 \(W\) 定义了流形上的Flow。RNN 的学习目标不是记住context，而是学到一个隐状态流形 \(S\) 及其上的演化规律，使得任务相关变量在 \(S\) 上形成低维、可预测的轨迹，且不相关扰动被压到法向方向并逐步衰减。</p> <p>RNN 的主要问题在于在每一步使用相同的权重矩阵 \(W\) 。这相当于强行假设流形是<strong>平坦的</strong>，即<strong>切空间在任何位置都相同</strong>。然而，真实的语义流形往往具有复杂的曲率。当序列很长时，真实的几何变换（雅可比矩阵的连乘）会因为曲率累积而导致特征值及其梯度的指数级爆炸或衰减。RNN 试图用一个固定的线性算子去逼近不断变化的切空间变换，这在数学上是病态的。</p> <p><strong>另一个思路</strong>：</p> <p>如果将时间 \(t\) 视为独立维度，RNN 将流形视为由参数 \(t\) 定义的高维欧几里得空间 \(R^{D+1}\) 中<strong>一条或多条静态曲线</strong>。隐状态 \(h_t\) 实际上是对流形上某一点 \((x_t,t)\) 处的切空间及其历史轨迹信息的编码。 而上述递归公式在几何上等价于沿着流形表面的曲线进行路径积分。RNN 试图定义一个向量场，通过沿 \(t\) 轴的切向量推进，逐步描绘出流形的形状。</p> <p>在这种静态空间中，RNN 强行假设流形是<strong>单连通</strong>且<strong>顺序依赖</strong>的。它必须严格沿着\(t\)的梯度方向遍历流形。如果流形在高维空间中存在卷曲使得 \(t_i\) 和 \(t_{i+k}\) 在欧氏距离上极近，但在测地线距离上极远，RNN 必须走完漫长的测地线才能建立联系。此外，由于是基于切向量的局部线性近似不断累积，一旦在某处的切空间估计出现偏差（梯度消失/爆炸），这种几何畸变会沿着路径指数级放大，导致对流形全局拓扑的认知坍塌。综上，RNN 将静态几何结构强行降维为一维路径问题，丢弃了流形在高维空间中的非局部几何特性。</p> <h3 id="4-mamba-ssm连续流形的优化控制"><strong>4. Mamba (SSM)：连续流形的优化控制</strong></h3> <p>Mamba（及其背后的S4/S6理论）是对RNN的几何修正。 它沿用了 Dynamical System 的视角，但引入了<strong>HiPPO矩阵理论和Selective Scan</strong>。</p> \[h′(t)=Ah(t)+Bx(t)\] \[y(t)=Ch(t)\] <p>HiPPO矩阵 \(A\) 的特殊构造，保证了状态\(h_t\)是过去所有输入流形历史在正交多项式基底上的最优投影。它解决了RNN“记不住”的问题。同时Mamba引入了 \(B(x)\) ， \(C(x)\) ， \(Δ(x)\) ，使得流形上的流场是输入 \(x\) 的函数。这将RNN这一线性时不变系统（LTI）扩展为了<strong>线性时变系统（LTV）</strong>。</p> <p>然而，即便做出了上述优化，整个系统的信息压缩依然是有损的，如果不考虑线性复杂度的优势的话，其在处理离散图匹配类型任务时往往效果不如Transformer，对非因果数据处理也不如Attention直观</p> <h3 id="5-transformer基于动态度量自适应图结构"><strong>5. Transformer：基于动态度量自适应图结构</strong></h3> <p>在研究初期，人们大多认为Transformer是序列模型，但如果忽略 \(PE\) 注入的时序信息，其本质是一个基于集合的动态图神经网络。Transformer将“数据流形”视为一个Complete Graph，由模型自己去学习边的权重。它不再受限于欧氏空间的邻域定义，因此能处理非网格数据。</p> <p><strong>自注意力：数据驱动的黎曼度量</strong></p> <p>在流形学习中，核心难题是如何定义流形上两点之间的距离。CNN假设了固定的欧氏距离（像素相邻即相关），RNN假设了时间距离。Transformer通过自注意力机制抛弃了这些固定度量，转而学习一个<strong>Data-Dependent Metric Tensor</strong>。其中的attention metrics实际上是在构建一个以内积为核函数（黎曼度量）的<strong>动态邻接矩阵</strong>。不同于欧氏距离的各向同性，Attention 是高度各向异性的。它根据Context动态调整切空间的方向，使得模型能够忽略序列位置上相距甚远但在语义流形上相邻的 Token。</p> <p>相比于CNN只能在流形上局部扩展和RNN的沿时序路径积分，Transformer可以通过Attention机制在流形上建立“虫洞”，直接连接测地距离极远的两个点。这消除了曲率累积带来的噪声，使得梯度可以在流形上无损传播。</p> <p><strong>多头机制：多重子流形投影</strong></p> <p>从流形几何角度，高维语义流形 \(M\) 往往是由多个Sub-manifolds的笛卡尔积组成的： \(M≈M_{syntax}×M_{semantic}×M_{tone}...\) 。例如一个 Head 可能捕捉句法结构的子流形（主谓宾关系），另一个 Head 可能捕捉指代关系的子流形（代词与其指代对象）。 Multi-Head Attention 允许模型在不同的切子空间并行地计算几何关系，最后通过线性投影拼接，恢复出完整的流形结构。</p> <p><strong>综上，Transformer的先验其实反而弱化了已有偏置。它付出了 \(O(N^2)\) 的计算代价，换取了捕捉任意拓扑结构流形的能力</strong>。这也是为什么它需要海量数据，即它必须从零开始学习流形的拓扑，而不能像CNN，RNN那样提前获得局部性先验</p> <h2 id="二scaling-law与涌现"><strong>二、Scaling Law与涌现</strong></h2> <p>从架构层面看，LLM（如 GPT-4, LLaMA）与最初的 Transformer并无本质区别，主要差异在于规模。流形假设的视角下，从小模型到大模型的转变，不仅是量的积累，更是流形拓扑结构、覆盖密度与连通性的质变。</p> <h3 id="1-scaling-law"><strong>1. Scaling Law</strong></h3> <p>在深度学习中，Loss 本质上衡量的是模型学习到的流形与真实数据流形之间的距离。Scaling Law 描述了 Loss 随计算量 \(C\)、参数量 \(N\)、数据量 \(D\) 的幂律下降关系： \(L(N)∝N^{−α}\) 。</p> <ul> <li> <strong>增加参数 \(N\)（降低 Bias）：</strong> Scaling Law 表明：随着 \(N\) 增加，模型拟合高频曲率的能力呈幂律上升。小模型只能学习流形的整体骨架，即流形的主成分。此时Loss下降快。随着 \(N\) 增加，模型开始能够包裹住流形上曲率极大的高频区域，即那些罕见的、复杂的长尾样本。</li> <li> <strong>增加数据 \(D\)（降低 Variance）：</strong> 根据覆盖数理论，为了以精度 \(ϵ\) 覆盖一个 \(d\) 维流形，所需样本数 \(M∝(1/ϵ)^d\) 。Scaling Law 实际上揭示了在特定的 \(d\) 下，逼近误差随样本密度增加的衰减速率。这解释了为什么图像生成（高 \(d\)）比文本分类（低 \(d\)）更难scale。Scaling Law 的存在证明了深度网络确实是在进行流形学习，而不是简单的记忆。如果是记忆，Loss曲线不会呈现这种幂律分布。</li> </ul> <p>从 Foundation Model 的角度，LLM 的训练过程是对语言流形的极致逼近。</p> <h3 id="2-涌现流形上的相变"><strong>2. 涌现：流形上的相变</strong></h3> <p>当我们将数据分布视为流形，学习过程则可被视为在流形上建立连通性的过程。</p> <ul> <li> <strong>小模型</strong>：模型学习了流形上分散的局部邻域，但在这些邻域之间没有建立正确的映射关系。此时模型无法完成多步推理，因为推理路径断裂。</li> <li> <strong>临界点</strong>：当参数量 \(N\) 和训练数据量 \(D\) 超过某个阈值，模型对流形的覆盖密度达到了渗流阈值。分散的局部知识突然连通成一个全局一致的图谱。</li> <li> <strong>涌现</strong>：此时，模型不仅能插值，还能在流形上进行传递性组合。例如，当获知 A→B 和 B→C 后涌现出了 A→C 的能力。这在宏观上表现为性能的突变（类似于物理学复杂系统中的相变）</li> </ul> <p>而涌现往往伴随着表征空间的几何重构（线性化解耦）。在相变前，不同概念在流形上是纠缠扭曲的，线性层无法分离。相变后，模型学会了将弯曲的流形展开到高维欧氏空间中，使得复杂的语义关系变得线性可分。这种展开能力的习得，往往需要达到一定的深度和宽度，这正是涌现发生的时刻。</p> <p><strong>为什么其他结构（CNN/RNN）无法实现同等程度的涌现</strong>？</p> <p>其根本原因依然是<strong>归纳偏置。</strong></p> <p>RNN 强制将所有历史信息压缩进一个固定维度的状态向量 \(h_t\)。对于复杂的流形轨迹，这相当于试图将高维流形投影到低维空间，必然导致<strong>信息丢失和Singularities</strong>。当序列变长，轨迹发散，模型无法通过有限的内存维持流形的全局几何结构，因此无法涌现出长程推理能力。CNN虽然Efficient，但其卷积核的感受野增长是线性的。要覆盖流形上两个相距甚远的关联点，需要堆叠极深的网络，导致优化困难。更重要的是，CNN的权值共享机制假设流形各处几何性质相同，这限制了其处理non-stationary语义流形的能力。</p> <p>此外，Transformer的Attention是输入与权重的乘积，甚至是输入自身的二次型（QKT）。这使得它本质上是一个二阶及以上的高阶网络，而其他架构多为一阶累加。高阶交互允许Transformer根据上下文动态调整计算权重。这种上下文学习能力是固定权重的RNN/CNN难以具备的。</p> <h2 id="三流形的高效建模"><strong>三、流形的高效建模</strong></h2> <p>即便通过弱化归纳偏置，Transformer获得了上下文自适应建模能力。但是这种自适应近表现为对于统一数据流形中的输入输出token的调整，而非为不同的数据分布/流形的模型层面的适配。如果我们有无限的人力，数据和计算资源，那么我们完全也可以对于每一个子任务都训练一个特定小规模的Transformer，而非用一个亿万参数大模型暴力拟合。其实现在很多对于Transformer的架构优化就是在隐式地做这件事，例如MoE是在为FFN中的记忆划分区域，Sparse/Gated Attention则是建立流形上坐标间的高度灵活且稀疏的链接。</p> <h3 id="1-moe"><strong>1. MoE</strong></h3> <p>MoE 的本质是承认用单一的、全局共享的Dense Model去逼近一个拓扑结构极其复杂、曲率变化剧烈的高维流形，在数学上是低效且不稳定的。如果真实数据分布由多个子流形组成，那么一个Dense Model是在学一个全局统一坐标系下的单一算子（类比二阶MLP）；而一个MoE 模型更像是在学多个局部算子（分别覆盖不同的子流形区域）外加一个路由（学习输入 token 属于哪个区域，以及应当调用哪些局部算子）。</p> <p><strong>MoE 为什么适合基座模型的长尾与多域</strong>？</p> <p><strong>长尾样本</strong>往往落在Dense Model很少访问的区域，等价于子流形的稀疏分支或小片段。同时，Dense Model在固定计算预算下，很难同时在所有区域维持高分辨率逼近。而 MoE 能把分辨率按需分配：让特定区域由特定专家承担，从而在同等 token 计算下提升对多域结构的覆盖。</p> <h3 id="2-sparse-attention"><strong>2. Sparse Attention</strong></h3> <p>传统Attention是在在 token 流形上建图并做扩散/核平滑。若 token 表征落在低维流形上，最有效的信息往往来自于流形上的近邻或少数跨区捷径。而全连接带来了大量非流形上的噪声连接（远距离、无语义关联的 Token 交互），因此有效的mixing并不需要对所有节点全连接。Sparse Attention 的核心就是把传统Attention中的全连接近似为稀疏图：</p> <ul> <li>Top-k / kNN 稀疏化：只保留每个 query 的 \(k\) 条最大相似度边（流形近邻）。</li> <li>块稀疏/局部窗口：先验假设近邻在序列位置上也局部（适合局部依赖）。</li> <li>预算自适应稀疏：不同 token、不同阶段（prefill vs decode）用不同稀疏度。</li> </ul> <h3 id="3-gated-attention"><strong>3. Gated Attention</strong></h3> <p>近期兴起的 Gated Attention则是跨越了对于注意力矩阵的微调（上述的微调往往伴随着新的归纳偏置的引入），直接对投影后的向量场进行自适应控制。Gated 机制通过引入门控信号 \(g∈[0,1]^d\) ，对更新向量进行逐维度的过滤。几何上，这是对切向量进行正交分解与抑制。Gate 识别并抑制掉那些垂直于当前任务子流形的噪声分量，仅保留沿测地线方向的有效分量。此外，门控机制作用于Q向量上最为有效，这来自于稀疏性带来的投影矩阵的rank的提升。</p> <p>在处理长序列时，Gated Attention 动态调整历史信息的衰减率且避免了Attention Sink。这等价于根据流形的局部曲率动态调整惯性。在流形平坦区域（语义连贯），Gate 允许信息长距离传输；在曲率突变区域（语义转换），Gate 迅速截断历史依赖，重置轨迹方向。</p> <p>上述这些架构优化其实超越了稳定训练过程或者是提升特征表示能力本身。因为更深层次的推理往往对应着<strong>更为抽象的embedding和单一embedding信息密度的升高</strong>，而MoE和Sparse/Gated Attention则为这样的高信噪比表征提供了结构性基础。当有了训练数据（更高质量）和训练方式（更深层次的RL）的支持，这种稀疏的类路由机制搭建的思维链/图才会展现出更大的潜力。从这个层面理解，<strong>思维的稀疏性一定程度上反映了模型的智能水平</strong>。</p> <h2 id="四真假reasoning"><strong>四、真假Reasoning</strong></h2> <h3 id="1-上下文学习-in-context-learning"><strong>1. 上下文学习 (In-Context Learning)</strong></h3> <p>当前LLM中的ICL本质上是对高维数据在低维拓扑结构中动态定位与插值过程的数学抽象。ICL 并非“学习”新知识，而是对预训练期间已构建的低维流形结构的检索、定位与局部线性化。</p> <p>Transformer 在推理阶段，通过 Prompt 中的示例（在几何上充当了流形上的锚点），在隐空间中动态定位到了一个特定的任务子流形。 Attention 机制实际上是在计算输入 Query 与这个子流形切空间的投影，即模型只是在已习得的庞大流形结构中，通过 Prompt 锁定了局部坐标系。</p> <p>Transformer 的前向推理过程，在数学形式上等价于在参数空间或激活空间上，针对上下文示例定义的损失函数 进行的梯度下降更新。给定上下文时，模型不需要更新物理权重W，而是通过改变内部激活状态，模拟了在 \(M_{task}\) 上寻找最优解的过程。这使得 ICL 表现为在流形特定区域内的局部微调。</p> <p>但是ICL无法进行真正的推理或创造新知识。它只能在预训练流形的凸包内或邻域进行插值。如果任务所需的逻辑完全正交于预训练流形，即 Out-of-Distribution，ICL 必将失效或产生幻觉。同时。随着上下文长度增加，如果示例存在冲突或噪声，会导致激活点在流形上定位不稳，甚至坍缩到错误的子流形区域。</p> <h3 id="2-cot显式符号的路径规划"><strong>2. CoT：显式符号的路径规划</strong></h3> <p>从流形假设的严谨视角来看，Chain of Thought并非是推理涌现，而是一种减小映射曲率与测地线插值的几何策略。</p> <p>在现有的LLM中，对于简单任务，输入 \(x\) 和输出 \(y\) 在流形上距离很近，或者流形在局部是平坦的。此时，单步推理 \(y=f(x)\) 是有效的，因为通过简单的线性插值或浅层非线性变换即可逼近。但是对于复杂任务，\(x\) 和 \(y\) 虽然都在流形上，但它们之间的测地线距离极远，且流形结构高度卷曲、非凸，甚至存在拓扑空洞。如果强行训练模型直接预测 \(P(y∣x)\) ，模型试图在环境空间中走直线连接 \(x\) 和 \(y\)。但这条直线往往穿过了流形之外的OOD区域，这导致了模型产生幻觉或逻辑断裂。</p> <p>CoT 强制模型在 \(x\) 和 \(y\) 之间生成中间步骤 \(z_1,z_2,…,z_n\) 。 这实际上是将一个极其困难的全局映射问题，分解为一系列低Lipschitz常数的局部映射问题，显式地消除了不确定性。CoT 生成的 Token 序列，就是流形表面上一条离散化的测地线路径。</p> <h3 id="3-latent-cot回归连续流形动力学"><strong>3. Latent CoT：回归连续流形动力学</strong></h3> <p>但是人类语言是思维流形的低维、量化投影。强迫模型输出自然语言来进行推理，相当于强迫连续的神经网络信号经过一个低带宽的 Argmax 离散化层。同时许多直觉、模糊的中间状态无法被离散词汇精确捕获。<strong>这种量化噪声在长链条推理中会累积，导致推理漂移</strong>。</p> <p>Latent CoT 试图移除离散Token的约束，直接在Latent Space进行轨迹演化。在这种情况下，在 Latent Space 中，模型可以维持既是A又是B的叠加态，直到推理的最后一刻才坍缩为离散输出。这避免了过早的离散化决策带来的错误传播。另外，一个 Token 只能传递 \(log_2∥V∥\) bits 的信息（ \(∥V∥\) 为词表大小），而一个 \(d\) 维 FP16 向量理论上可以传递的信息量远超于此。Latent CoT 利用了这个高维宽带通道来传递复杂的推理状态。</p> <p>这种内部思考实际上是在增加计算图的深度。从流形角度看，这是通过多次复合函数变换，将原本纠缠在一起的流形逐步拉伸、解开，直到线性可分。</p> <p>对于 Latent CoT，由于没有显式的 Ground Truth 文本，训练方法正从SFT转向RL，即目标函数不再是预测下一个 Token，而是最大化最终答案的准确率。模型被鼓励自由探索隐空间中的路径。研究发现，模型学出的 Latent Thoughts 往往呈现出人类无法理解的特征分布——这恰恰证明了它突破了人类语言的低维流形限制，找到了更优的Shortcuts。</p> <p><strong>但是，基于思维链的推理是通用推理么？</strong></p> <p>目前的LLM表现出的推理，在数学上主要是高维流形上的<strong>局部插值</strong>。Scaling Law 的本质是随着参数增加，流形的采样密度呈指数级上升，使得测试样本落入训练样本 \(ϵ\)-邻域的概率无限趋近于1。而人类的思维具备离流形外推的能力。人类可以在数据流形的空洞处，或者完全正交于流形的方向上建立新的逻辑连接。</p> <p>另外，人类语言是物理世界的一个<strong>低维、有损、高度压缩的投影</strong>。 人类鲜少仅仅使用语言来扩展对于真实世界的理解，即便是从数学/物理推算得到的定理与猜想，也需要现实世界中的实验验证后才会被广泛接受。而LLM 试图通过 \(M_{language}\) 重构 \(M_{world}\)，即建立世界模型。但由于这种投影不是双射，从语言逆推世界存在无穷多解。因此与环境的交互仍是不可或缺的。</p> <p>最后，现有的大模型即便自适应也仍然是静态模型，当数据分布（流形结构）变化时，不经历重训或微调仍然会触及OOD的屏障，而重训和微调造成的灾难性遗忘仍然是难以跨越的阻碍（因此最近continue learning愈发火热）。不具备动态，则人类注定要在模型的反复训练中烧掉天量的资金和电力。</p> <h2 id="结语"><strong>结语</strong></h2> <p>码字太多不想再啰嗦。</p> <p>一言以蔽之，在追寻热点的浪潮中不要忘记，金子大多早已存在在来时的阴影中。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/24/spacedrive.html">SpaceDrive：为自动驾驶VLA注入空间智能</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/24/spacedrive.html">SpaceDrive：Infusing Spatial Awareness into VLM-based Autonomous Driving</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/15/manifold-hypothesis-2.html">In the Perspective of Manifold Hypotheses - 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/11/25/manifold-hypothesis-1.html">流形假设(Manifold Hypothesis)下的思考·一</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/11/25/manifold-hypothesis-1.html">Reflections on the Manifold Hypothesis - 1</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Peizheng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>