<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 流形假设(Manifold Hypothesis)下的思考·一 | Peizheng Li </title> <meta name="author" content="Peizheng Li"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?343676cba58161999546fb5ac8b9e53d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://edwardleelpz.github.io/%E4%B8%AD%E6%96%87/2025/11/25/manifold-hypothesis-1.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Peizheng</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">流形假设(Manifold Hypothesis)下的思考·一</h1> <p class="post-meta"> Created on November 25, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> Tech-Blog   ·   <i class="fa-solid fa-tag fa-sm"></i> 中文 </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="引言高维噪声中的几何秩序"><strong>引言：高维噪声中的几何秩序</strong></h2> <p>受到Kaiming大佬最近的的JiT启发，想聊聊在生成领域被广泛讨论的流形假设。类似的讨论虽然在生成领域已经屡见不鲜，但是其实际上是提供了一个理解现有深度学习架构的几何视角，可以潜在泛化到不同的任务和场景当中。</p> <p><strong>流形假设（Manifold Hypothesis）</strong> 是深度学习理论的基石之一。它试图解释神经网络能够在极高维的数据（如图像、文本、音频）上的出色表现。简单来说，流形假设认为：<strong>虽然现实世界的数据（如一张图片）在表面上由成千上万个维度 \(D\) （像素）组成，但它们实际上分布在一个内嵌入于高维空间中的低维（维度为 \(d\) 且 \(d\) 远小于 \(D\) ）流形上。</strong></p> <p>在数学拓扑学中，流形在局部近似欧几里得空间（比如平面或直线），但整体上可能有复杂结构的几何形状。为了更直观的理解，可以想象一张二维的纸（2D平面）：纸本身是二维的，由 \((u,v)\) 坐标描述。但若将其卷曲揉搓成一团，放在三维空间里中，对于观察者来说这个物体则存在于3D空间中，由 \((x,y,z)\) 坐标描述。但对于纸上的一只位于 \((u,v)\) 的蚂蚁来说，它仍然是一个二维世界。这里，这张卷曲的纸便对应了高维现实空间中数据所在的低维流形。</p> <p>流形假设在深度学习语境下包含以下几个关键推论：</p> <p><strong>1. 数据的自由度很低</strong></p> <p>数据的表观维度虽然庞大，但受现实条件约束，其真实自由度其实很少。以图像为例，一张人脸照片可能有几百万个像素，但这张图片的变化自由度其实很少，比如人脸的角度（上下左右）、光照的方向、面部表情（张嘴/闭嘴）等等。这些自由度构成了数据的<strong>固有维度</strong>。这意味着模型可能只需要几十个维度便可以精准刻画数据特征。</p> <p><strong>2. 神经网络的作用是展开流形</strong></p> <p>分类与回归任务的本质是处理流形的纠缠。在原始高维空间中，不同类别的流形可能像打结的绳子一样纠缠在一起，线性不可分。<strong>深度神经网络（尤其是深层结构）被认为是在对空间进行非线性的扭曲和拉伸</strong>。通过逐层的坐标系转换，卷曲的、纠缠的流形慢慢展开，直到其变得平坦且线性可分。</p> <p>沿用以上纸张的比喻，相当于有两张揉皱并缠在一起的纸（纠缠的流形）。神经网络的工作就是通过一系列精准的展开（层与层之间的非线性变换），小心翼翼地把它们抚平并分开，最后在中间画一条线（分类）。</p> <p>流形假设不仅仅是理论猜测，也在实际操作中得以验证：</p> <p><strong>1. 隐空间插值（Latent Space Interpolation）</strong></p> <p>如果在像素空间对两张人脸图片 A 和 B 进行线性插值（0.5A + 0.5B），则通常会得到一张重影的、不自然的图像，因为叠加后的数据离开了流形，到达了高维空间的空旷区域。</p> <p>但是，如果使用生成对抗网络（GAN）或变分自编码器（VAE），将图片映射到低维的隐空间后插值并解码回来，则会看到人脸 A 平滑、渐变地变成了人脸 B。这证明了数据确实形成了一个连续的低维流形，流形上的坐标迁移反映了数据特征的转变。</p> <p><strong>2. 对抗样本（Adversarial Examples）</strong></p> <p>对抗攻击通常是通过在图像上添加微小的扰动，含有流形法线方向的位移，使数据点脱离支撑集或穿越决策边界，导致模型分类错误。这反向说明了模型学习到的决策边界是紧贴着数据流形的，但在流形法向方向上极度脆弱。</p> <h2 id="一-深度学习范式的几何演进从拓扑解结到统一场论">一、 深度学习范式的几何演进：从拓扑解结到统一场论</h2> <p>当我们审视深度学习的不同范式时，会发现它们实际上是在以不同的方式操作和利用同一个流形结构。</p> <h3 id="1-判别任务classification--regression流形的解纠缠与分离"><strong>1. 判别任务（Classification / Regression）：流形的解纠缠与分离</strong></h3> <p>在原始像素空间中，不同类别的样本往往位于高度卷曲且相互纠缠的流形上，线性分类器无法将其分离。判别模型通过层级化的非线性变换，充当了拓扑学中的同胚映射算子。它不改变数据的拓扑性质，但通过拉伸和扭曲空间，忽略流形的内部度量结构，专注于最大化类间流形的距离。这就像将一团乱麻中的红纸和蓝纸解开，直至在高层特征空间中，它们变得平坦且线性可分。</p> <h3 id="2-生成任务generative-modeling流形的参数化与遍历"><strong>2. 生成任务（Generative Modeling）：流形的参数化与遍历</strong></h3> <p>生成模型关注的是流形本身的几何形状与概率密度。以扩散模型（Diffusion Models）为例，其去噪过程可被视为在高维能量景观中寻找流形的过程。模型学习了一个能够指示“距离流形有多远”的分数函数（Score Function/Gradient Field）。生成过程就像是一个随机落入高能高维空间噪点，沿梯度场轨迹收敛至低维流形表面的过程。</p> <h3 id="3-统一架构序列流形上的能量最小化"><strong>3. 统一架构：序列流形上的能量最小化</strong></h3> <p>当前最前沿的趋势是将判别与生成统一于同一架构，例如基于 Transformer 的 Next-Token Prediction。在流形视角下，这两者不再有本质区别，皆为序列流形上的轨迹延伸。<br> 无论是输入“这张图是什么”（判别）还是“请画一只猫”（生成），模型都在给定起点的基础上，在流形表面规划一条使得能量函数最小化的路径。分类标签仅仅是流形上一种特殊的离散节点，而生成内容则是流形上的连续路径。这种视角消解了任务边界：大语言模型实际上是在构建一个联合语义流形，通过预测下一个 Token 来逼近流形的切空间方向。</p> <p>统一架构的优势主要在于<strong>流形约束互补（流形正则化）</strong>：纯判别模型只需要关注流形的<strong>边界</strong>而忽略流形<strong>内部的结构</strong>，导致对对抗样本脆弱；纯生成模型关注流形<strong>内部结构</strong>但对<strong>类间差异</strong>理解不足。而<strong>统一架构的优势</strong>在于强迫模型既要能画出流形（生成），又要能区分流形（判别），这迫使模型学习到的流形表示<strong>既精准（贴合数据分布）又鲁棒（边界清晰）</strong>。</p> <h3 id="4-多模态模型异构流形的同胚对齐"><strong>4. 多模态模型：异构流形的同胚对齐</strong></h3> <p>图像流形是连续的像素变化，文本流形是离散的符号组合。CLIP 或 GPT-4o 等多模态模型的核心挑战，在于寻找这两个异构几何体之间的共形映射。流形假设在此引入了一个核心概念：<strong>共享语义流形（Shared Semantic Manifold）</strong>。图像和文本仅仅是同一语义实体不同投影。多模态学习旨在构建一个共享的潜空间（Latent Space），使得“猫的图像”与“Cat”的词向量在该空间中重合，从而实现语义的统一。</p> <p>此外，凭借着统一架构的优势，简单的模态间语义对齐进一步发展成为跨模态理解。这种情况下，“Cat”这一文本token除了对应于猫这一<strong>语义锚点</strong>本身，也会更加贴近于诸如面部的胡须，身上的绒毛的low-level视觉表征。则进一步填满了不同模态流形之间的缺口，使共享语义流形更为平滑且完整。因此，多模态大模型在理解能力上往往优于单模态专家模型。</p> <h2 id="二-深度强化学习动态流形上的控制与规划"><strong>二、 深度强化学习：动态流形上的控制与规划</strong></h2> <p>当引入时间维度与决策机制，流形假设便从静态几何延伸至动态系统理论，这在深度强化学习（DRL）中体现得尤为深刻。与监督学习处理“静态”数据不同，DRL 处理的是动态的、交互式的数据流。</p> <h3 id="1-状态空间的物理降维"><strong>1. 状态空间的物理降维</strong></h3> <p>DRL 中的 Encoder 承担着<strong>将高维观测（如百万维的视频流）投影到低维物理状态流形</strong>的任务。这个流形的维度通常等同于系统的物理自由度（如关节角度、位置坐标）。有效的表征学习必须过滤掉光照、纹理等与动力学无关的“噪声维度”，仅保留决定系统演化的本质坐标。如果Encoder没有正确学到流形结构，它可能因为背景中一个无关像素的改变（在像素空间距离很远，但在物理流形上位置没变），就给出截然不同的动作预测。</p> <h3 id="2-动力学作为向量场"><strong>2. 动力学作为向量场</strong></h3> <p>物理定律在流形上定义了严格的<strong>向量场（Vector Field）或流（Flow）</strong>。Agent的策略Policy并非在真空中选择动作，而是在流形表面受到动力学约束的可行性管道（Feasible Tube）中规划测地线。状态不能在流形上随意跳跃，只能沿着动力学方程允许的方向移动。训练过程即是寻找一条通往高回报区域的最优曲线。</p> <h3 id="3-黎曼流形与自然梯度natural-gradient--trpo--ppo"><strong>3. 黎曼流形与自然梯度（Natural Gradient / TRPO / PPO）</strong></h3> <p>通常我们优化神经网络参数 \(θ\) 。但在 RL 中，我们真正关心的是 <strong>策略分布 \(πθ(a∣s)\)</strong> 的变化。但是在参数空间 \(θ\) 中走一小步（欧几里得距离），可能会导致策略分布发生剧烈变化，即导致 Policy Collapse，策略崩溃。参数空间并不是描述策略变化的正确几何空间。</p> <p>而当我们考虑策略分布时，其构成了一个<strong>统计流形</strong>。在这个流形上，测距尺度从欧几里得距离变为了 <strong>KL 散度</strong>。当使用TRPO时，TRPO 限制每次更新时，策略在<strong>流形上的移动距离</strong>（KL 散度）不能超过一个阈值。这就像在弯曲的地球表面行走，TRPO 确保模型沿着测地线走，而不是直接穿过地心。这解释了为什么 PPO/TRPO 比普通的 Policy Gradient 更稳定：它们尊重了策略空间的流形几何结构。</p> <h3 id="4-流形边界与分布外泛化ood"><strong>4. 流形边界与分布外泛化（OOD）</strong></h3> <p>当 Agent 不能与环境交互，只能从历史数据中学习时，流形假设解释了最大的难题：<strong>分布外误差（OOD Error）</strong>。历史数据集包含的轨迹只覆盖了整个状态流形的一小部分，即<strong>行为流形（Behavior Manifold）</strong>。整个环境可能包含巨大的状态空间，但数据只是一条细线。</p> <p>离线强化学习（Offline RL）中的 OOD 问题，本质上是策略轨迹掉出了数据流形。在流形外部的区域，价值函数Q-function由于缺乏支撑集，往往会产生高估的幻觉。因此，现代 RL 算法的核心在于通过约束策略，使其紧贴已知流形的表面，避免滑入未知的几何深渊。</p> <h2 id="三-通用智能所需要的流形是什么样的"><strong>三、 通用智能所需要的流形是什么样的</strong></h2> <p>基于流形假设的视角，现有的模型（如 GPT-4 或 Stable Diffusion）大多是在<strong>已知的、固定的</strong>流形上进行插值或模式匹配。而通用人工智能需要的模型，必须具备驾驭、扩展甚至创造流形的能力。</p> <h3 id="1-全局一致的因果世界模型"><strong>1. 全局一致的因果世界模型</strong></h3> <p>AGI 需要的是一个<strong>统一的、全模态的超流形（Hyper-Manifold），其</strong>需要：</p> <ul> <li>构建一个<strong>底层的、与模态无关的潜在空间</strong>。在这个空间里，无论是看到“苹果”的图像、听到“Apple”的声音、还是感受到牛顿的引力公式，它们都映射到同一个物理实体的<strong>本质几何结构</strong>上。</li> <li>不仅限于相关性，而是包含<strong>时间箭头</strong>和<strong>因果图</strong>结构。这意味着流形上的路径是<strong>有向的</strong>。在流形上，A→B 是可行的轨迹，但 B→A 可能是被物理定律等流形边界禁止的。</li> </ul> <h3 id="2-agi-的核心能力在流形上的推理与跳跃"><strong>2. AGI 的核心能力：在流形上的推理与跳跃</strong></h3> <p>深度学习擅长直觉（平滑插值），但不擅长逻辑（外推）。例如，数学推理往往需要多步精确推导，任何一步滑出流形都会导致错误。AGI 模型需要能够识别流形上的<strong>关键节点</strong>，并通过逻辑规则在这些节点之间建立<strong>Shortcuts</strong>。此外它还需具备想象力（例如反事实推理）要回答“如果……会怎样？”。这要求模型能够在流形上人为地通过干预改变某个变量，然后推演新的轨迹，即使这个轨迹从未在训练数据中出现过。</p> <h3 id="3-核心困境稀疏观测与不适定问题"><strong>3. 核心困境：稀疏观测与不适定问题</strong></h3> <p>迈向通用人工智能的根本障碍在于数据的稀疏性。相对于真实物理世界的无限可能性，人类所能收集的数据在数学上是<strong>测度为零（Measure Zero）</strong>的。这就引出了一个经典的<strong>不适定问题（Ill-posed Problem）</strong>：</p> <p>想象在一张白纸上画 3 个黑点，大致排成一条弧线。模型得到的任务是：“画出这就 3 个点所在的真实曲线。”如果没有任何先验知识，只看这三个点，模型会面临<strong>无穷多种可能性</strong>：</p> <ul> <li> <strong>可能性 A（平滑）</strong>：一条平滑的抛物线。</li> <li> <strong>可能性 B（震荡）</strong>：一条剧烈震荡的折线，恰好在拐点穿过了这三个点。</li> <li> <strong>可能性 C（复杂）</strong>：一个简笔画的“猫”的轮廓，这三个点刚好是猫的耳朵和尾巴。</li> <li> <strong>可能性 D（断裂）</strong>：根本不是一条连续的曲线，而是三个独立的、毫无关系的孤岛。</li> </ul> <p>在没有额外假设的情况下，连接这三个点的线有无数条。仅仅依靠数据点，<strong>真实的流形是不可知的</strong>，这就是所谓的不适定问题。</p> <p>人类解决这个问题的策略主要依靠以下几种直觉：</p> <ul> <li> <strong>奥卡姆剃刀原则</strong>：偏好平滑与简单：我们通常认为<strong>世界是简单的、连续的、渐变的</strong>。如果没有证据表明这根线在疯狂抖动，则默认它是平滑的。<strong>在流形学习中</strong>，这对应了正则化，即强迫模型寻找最简单的那个流形。</li> <li> <strong>物理世界的先验模板</strong>：我们往往透过点本身来猜测其物理意义。如果已知这三个点是“扔出去的球在空中的位置”，那么最大可能性应该是抛物线。因为重力定律用已知的物理方程（ \(y=ax^2+bx+c\) ）约束了流形的形状。如果已知这三个点是“一支股票连续三天的价格”，那么折线的可能性则非常大。因为在知识先验中，金融流形是分形的、不平滑的。</li> <li> <strong>语义补全</strong>：在上述案例中，如果假设这三个点排成一个三角形，下面还有一条横线，那么很大概率上人类会判断这是一张脸。而这本质不是在拟合曲线，是在<strong>检索记忆中的流形</strong>。人类的大脑里存储了成千上万种“物体流形”的压缩包。当你看到这几个点时，“脸”这个高维流形被激活和投影，完美地通过了这几个点。</li> </ul> <p>单纯依赖数据拟合，模型无法确定哪一条才是符合真实物理法则的“真流形”。这意味着，仅靠扩大数据规模，永远无法通过插值完全覆盖高维空间的复杂动态，外推必然失败。通用的 AI 如果想在数据稀缺的情况下还原真实世界流形，它不能只做一个“数据拟合器”，它必须拥有像人类一样的<strong>元知识库</strong>。</p> <h2 id="四-归纳偏置的必然性与选择"><strong>四、 归纳偏置的必然性与选择</strong></h2> <p>在上述的讨论中，无论是先验模板还是语义逻辑，甚至是奥卡姆剃刀原则这中优化逻辑，其本质依然是<strong>归纳偏置（Inductive Bias）</strong>。虽然归纳偏置在很多时候都是局限于某一场景或模型，也是很多时候我们想要消除的核心（例如自监督学习），但是事实上<strong>并不存在所谓的“无偏见”模型</strong>。</p> <h3 id="1-为什么无法消除归纳偏置"><strong>1. 为什么无法消除归纳偏置？</strong></h3> <p>No Free Lunch：如果对数据分布没有任何先验假设，任何学习算法的期望性能都与随机猜测无异。也就是说，如果不做出某种假设，比如未来可能和过去相似或相邻的点可能有相似的值，模型就无法从有限的数据中通过逻辑推导出任何关于未知的结论。<strong>归纳偏置不是算法的缺陷，而是学习能够发生的先决条件。没有偏置，模型面对稀疏数据时将陷入不可知论的瘫痪</strong>。我们无法消除 Bias，我们只能选择 Bias。问题的关键不在于“是否有 Bias”，而在于“哪个 Bias 的普适性最强、被证伪的风险最低”。GPT的成功，某种意义上时我们选择或者搜寻到了对于人类语言来说最优（至少截止目前来看）的objective。</p> <h3 id="2-猜测什么样的偏置是必要的"><strong>2. 猜测：什么样的偏置是必要的？</strong></h3> <p>并非所有的偏置都是有益的,简单的语义偏置容易被反例证伪。一种简单的猜测是：通向 AGI 的模型需要的是元先验（Meta-Priors），即关于宇宙底层运行规律的几何与物理约束：</p> <ul> <li> <strong>物理对称性：</strong> 将平移、旋转、时间不变性硬编码或软约束进模型架构。这相当于告诉模型，无论流形如何卷曲，它必须遵守守恒定律。</li> <li> <strong>因果稀疏性：</strong> 假设流形背后的生成图是稀疏的。这强迫模型解耦变量，避免建立虚假的全连接相关性。</li> <li> <strong>算法极简性：</strong> 依据奥卡姆剃刀原则，倾向于选择生成复杂度最低的流形结构（即寻找最短的动力学方程）。</li> </ul> <h2 id="五-动态修正从静态先验到贝叶斯流形演化"><strong>五、 动态修正：从静态先验到贝叶斯流形演化</strong></h2> <p>虽然元先验极其稳健，但任何预设的偏置都有可能在特定环境下失效。因此，AGI 模型不能依赖僵化的先验，而必须具备<strong>动态修正流形结构</strong>的能力。</p> <h3 id="1-预测编码与误差驱动"><strong>1. 预测编码与误差驱动</strong></h3> <p>模型不应只是被动拟合，而应成为主动的预测者。通过自监督学习（如预测下一个 Token 或下一帧），模型构建当前的流形假设并进行推演。当预测结果与观测数据发生剧烈冲突时，这个误差信号不应仅仅用来调整参数，更应成为重构流形拓扑的动力。</p> <h3 id="2-贝叶斯流形与多假设维持"><strong>2. 贝叶斯流形与多假设维持</strong></h3> <p>在数据稀疏阶段，模型不应坍缩到单一的流形解释上，而应维护一个流形族的概率分布（Bayesian Manifold）。当新数据出现时，通过贝叶斯更新（Bayesian Updating）调整不同流形假设的后验概率。</p> <h3 id="3-主动干预"><strong>3. 主动干预</strong></h3> <p>要区分相关性流形与因果流形，模型必须具备和所处环境交互的能力，即通过干预来物理碰撞世界。这种交互数据是唯一能从根本上证伪错误流形结构的试金石。甚至对于动物和人类来讲，在物理世界中存活的能力和时间一定程度上反映了我们的智能水平。</p> <h2 id="结语"><strong>结语</strong></h2> <p>从流形假设出发，我们所需要的不再是一个单纯的大数据统计拟合器，基于人类提出的各种归纳偏置来overfit个别任务，在benchmark上刷出多高的性能。真正重要的也许是一个<strong>内嵌几何元先验、具备因果推理能力、且能通过预测误差动态重构自身结构的可微分世界模拟器</strong>。在这个视角下，学习的本质不是记忆数据在流形上的位置，而是捕捉生成这个流形的微分方程。而下一个阶段的AI，应该可以在动态变化的流形中生存下来，这种生存本身需要的能力，可能才是最为核心的智能。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/24/spacedrive.html">SpaceDrive：为自动驾驶VLA注入空间智能</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/24/spacedrive.html">SpaceDrive：Infusing Spatial Awareness into VLM-based Autonomous Driving</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/15/manifold-hypothesis-2.html">流形假设(Manifold Hypothesis)下的思考·二</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/15/manifold-hypothesis-2.html">In the Perspective of Manifold Hypotheses - 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/11/25/manifold-hypothesis-1.html">Reflections on the Manifold Hypothesis - 1</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Peizheng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>