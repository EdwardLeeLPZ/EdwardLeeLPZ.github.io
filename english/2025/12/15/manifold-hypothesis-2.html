<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> In the Perspective of Manifold Hypotheses - 2 | Peizheng Li </title> <meta name="author" content="Peizheng Li"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?343676cba58161999546fb5ac8b9e53d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://edwardleelpz.github.io/english/2025/12/15/manifold-hypothesis-2.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Peizheng</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">In the Perspective of Manifold Hypotheses - 2</h1> <p class="post-meta"> Created on December 15, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> Tech-Blog   ·   <i class="fa-solid fa-tag fa-sm"></i> English </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction-architectural-evolution-and-the-path-traveled"><strong>Introduction: Architectural Evolution and the Path Traveled</strong></h2> <p>For a long time, we have anticipated a next-generation architecture sufficient to replace the Transformer or the existing Attention mechanism. Whether it is the Mamba series from the past two years, aiming to subvert the Transformer entirely, or the recently discussed Sparse/Gated Attention based on iterative optimization of existing mechanisms, research continues to advance, yet the goal remains elusive. Compared to the unpredictable future, perhaps the path we have traveled offers greater clarity.</p> <h2 id="i-from-mlp-to-transformer"><strong>I. From MLP to Transformer</strong></h2> <p>From the perspective of the Manifold Hypothesis, all classical architectures can be viewed as topological transformations from a curled manifold to a flat feature space, each carrying different Inductive Biases.</p> <h3 id="1-mlp-the-universal-manifold-operator-with-no-structural-priors"><strong>1. MLP: The Universal Manifold Operator with No Structural Priors</strong></h3> <p>The MLP treats the input space \(R^D\) as a flat Euclidean space, utilizing hierarchical affine transformations and non-linear activation functions to indiscriminately fold, twist, and stretch the entire input space. According to the Universal Approximation Theorem, given sufficient neurons, an MLP can approximate any continuous function. Theoretically, this implies it can transform a manifold of any topological structure into a linearly separable form.</p> <ul> <li> <p><strong>The Worst Operator</strong>: The MLP structure embeds almost no Inductive Bias. Its core assumption is “smoothness”—that similar inputs yield similar outputs. Mathematically, this requires the learned function to satisfy the Lipschitz continuity condition: \(∥f(x)-f(y)∥≤K∥x−y∥\). However, in high-dimensional space \(R^D\), without specific geometric priors, guaranteeing this smoothness requires a sample size \(N\) that grows exponentially with dimension \(D\). Furthermore, the metric based on Euclidean distance \(∥x−y∥_2\) fails in high-dimensional space, as distances between all point pairs tend to converge. This <strong>assumption of global spatial isotropy</strong> ignores the complexity of high-dimensional manifold structures and is the root cause of its inefficiency. Consequently, massive use of MLPs often leads to suboptimal performance.</p> </li> <li> <p><strong>The Best Operator</strong>: Conversely, the near-absence of Inductive Bias means MLP has no requirements for input data distribution, possessing the capability to fold and project all manifolds. As a rigid transformation operator, MLP does not alter the data’s topological structure. Therefore, in cross-modal feature fusion (direct alignment of embeddings) and most downstream heads (where the feature space is sufficiently flat), MLP becomes the optimal (and most universal) choice. This has led to minimalist approaches like LLaVA.</p> </li> </ul> <h3 id="2-cnn-introduction-of-euclidean-group-symmetry"><strong>2. CNN: Introduction of Euclidean Group Symmetry</strong></h3> <p>The success of CNNs stems from explicitly exploiting the <strong>Geometric Priors</strong> of natural image manifolds:</p> <ul> <li> <p><strong>Local Connectivity</strong>: Assumes the manifold possesses local topological structure; pixel correlation decays with distance.</p> </li> <li> <p><strong>Weight Sharing</strong>: Assumes the geometry of the tangent space is uniform across the manifold. The manifold \(M\) is invariant under the translation group \(SE(2)\), i.e., \(f(T(x))=f(x)\) (invariance) or \(f(T(x))=T′(f(x))\) (equivariance).</p> </li> </ul> <p>As a spatially local operator, CNN explicitly hard-codes the <strong>symmetry</strong> of the manifold into the network structure. It performs “low-pass filtering” in the local neighborhood of the input manifold, attenuating high-frequency noise and unstable directions, thereby making the representation closer to the low-dimensional structure. This can be approximated as CNN performing multiple equidistant foldings and distortions on the entire manifold simultaneously.</p> <p>However, CNNs retain the Euclidean geometric metric. When the data manifold has non-zero curvature (e.g., spherical projection data, non-Euclidean graph data), the translation invariance assumption fails. CNNs cannot effectively capture long-range dependencies because, on a curved manifold, the “straight line” shortest path becomes a geodesic, which fixed-size convolution kernels cannot cover.</p> <h3 id="3-rnn-dynamical-system-trajectories-on-state-manifolds"><strong>3. RNN: Dynamical System Trajectories on State Manifolds</strong></h3> <p>RNN models the data manifold as a Dynamical System evolving over time:</p> \[h_t=σ(W_h h_{t−1} + W_x x_t)\] <p>This is essentially the discretized Euler integration of the ordinary differential equation \(\frac{dh}{dt}=f(h,x)\). The RNN is a temporal recursive operator attempting to learn the vector field of the manifold’s tangent space. Each hidden state \(h_t\) is a coordinate point on the manifold, and the weight matrix \(W\) defines the Flow on the manifold. The learning objective of RNN is not to memorize context, but to learn a hidden state manifold \(S\) and its evolutionary laws, such that task-relevant variables form low-dimensional, predictable trajectories on \(S\), while irrelevant perturbations are compressed into the normal direction and gradually attenuated.</p> <p>The primary issue with RNN is the use of the same weight matrix \(W\) at every step. This enforces an assumption that the manifold is <strong>flat</strong>, i.e., the <strong>tangent space is identical at every location</strong>. However, real semantic manifolds often possess complex curvature. When sequences are long, the actual geometric transformation (the product of Jacobian matrices) leads to exponential explosion or decay of eigenvalues and gradients due to curvature accumulation. RNN attempts to approximate a continuously changing tangent space transformation with a fixed linear operator, which is mathematically ill-posed.</p> <p><strong>Alternative Perspective</strong>:</p> <p>If time \(t\)is viewed as an independent dimension, RNN treats the manifold as <strong>one or more static curves</strong> in a high-dimensional Euclidean space \(R^{D+1}\)defined by parameter \(t\). The hidden state \(h_t\) effectively encodes the tangent space and historical trajectory information at a point \((x_t,t)\) on the manifold. The recursive formula above is geometrically equivalent to path integration along a curve on the manifold surface. RNN attempts to define a vector field that, by advancing via tangent vectors along the \(t\) axis, progressively delineates the manifold’s shape.</p> <p>In this static space, RNN forcibly assumes the manifold is <strong>simply connected</strong> and <strong>sequentially dependent</strong>. It must traverse the manifold strictly along the gradient direction of \(t\). If the manifold is curled in high-dimensional space such that \(t_i\) and \(t_{i+k}\) are extremely close in Euclidean distance but far apart in geodesic distance, RNN must traverse the entire lengthy geodesic to establish a connection. Moreover, relying on the continuous accumulation of local linear approximations of tangent vectors means that once a tangent space estimation deviates at any point (gradient vanishing/exploding), this geometric distortion is amplified exponentially along the path, leading to a collapse in the cognition of the manifold’s global topology. In summary, RNN forcibly reduces a static geometric structure to a one-dimensional path problem, discarding the non-local geometric properties of the manifold in high-dimensional space.</p> <h3 id="4-mamba-ssm-optimal-control-on-continuous-manifolds"><strong>4. Mamba (SSM): Optimal Control on Continuous Manifolds</strong></h3> <p>Mamba (and the underlying S4/S6 theory) is a geometric correction to RNN. It retains the Dynamical System perspective but introduces <strong>HiPPO Matrix Theory and Selective Scan</strong>.</p> \[h′(t)=Ah(t)+Bx(t)\] \[y(t)=Ch(t)\] <p>The special construction of the HiPPO matrix \(A\) ensures that the state \(h_t\) is the optimal projection of all past input manifold history onto an orthogonal polynomial basis. It solves the RNN’s “forgetting” problem. Simultaneously, Mamba introduces \(B(x)\), \(C(x)\), and \(Δ(x)\), making the flow field on the manifold a function of input \(x\). This extends the RNN from a Linear Time-Invariant (LTI) system to a <strong>Linear Time-Variant (LTV)</strong> system.</p> <p>However, even with these optimizations, the system’s information compression remains lossy. Disregarding the advantage of linear complexity, its effectiveness in discrete graph matching tasks is often inferior to the Transformer, and its handling of non-causal data is less intuitive than Attention.</p> <h3 id="5-transformer-adaptive-graph-structure-based-on-dynamic-metrics"><strong>5. Transformer: Adaptive Graph Structure Based on Dynamic Metrics</strong></h3> <p>Initially, the Transformer was viewed as a sequence model, but ignoring the temporal information injected by \(PE\), it is essentially a set-based dynamic Graph Neural Network. The Transformer treats the “data manifold” as a Complete Graph, where the model learns the edge weights itself. It is no longer constrained by neighborhood definitions in Euclidean space, enabling it to handle non-grid data.</p> <p><strong>Self-Attention: Data-Dependent Riemannian Metric</strong></p> <p>In manifold learning, the core challenge is defining the distance between two points on the manifold. CNN assumes a fixed Euclidean distance (pixel adjacency implies correlation), and RNN assumes temporal distance. The Transformer discards these fixed metrics via the self-attention mechanism, learning a <strong>Data-Dependent Metric Tensor</strong>. The attention metrics essentially construct a <strong>dynamic adjacency matrix</strong> using the inner product as a kernel function (Riemannian metric). Unlike the isotropy of Euclidean distance, Attention is highly anisotropic. It dynamically adjusts the direction of the tangent space based on Context, allowing the model to ignore Tokens that are distant in sequence position but adjacent on the semantic manifold.</p> <p>Compared to CNN’s local expansion and RNN’s path integration, the Transformer can establish “wormholes” on the manifold via the Attention mechanism, directly connecting two points with extreme geodesic distances. This eliminates noise caused by curvature accumulation, allowing gradients to propagate losslessly across the manifold.</p> <p><strong>Multi-Head Mechanism: Multiple Sub-Manifold Projections</strong></p> <p>From a manifold geometry perspective, a high-dimensional semantic manifold \(M\) is often the Cartesian product of multiple Sub-manifolds:\(M \approx M_{syntax} \times M_{semantic} \times M_{tone}...\). For example, one Head might capture the sub-manifold of syntactic structure (subject-verb-object relations), while another captures the sub-manifold of coreference resolution (pronouns and their referents). Multi-Head Attention allows the model to compute geometric relations in different tangent subspaces in parallel, finally recovering the complete manifold structure via linear projection and concatenation.</p> <p><strong>In summary, the Transformer’s prior actually weakens existing biases. It pays a computational cost of \(O(N^2)\) in exchange for the ability to capture manifolds of arbitrary topological structure.</strong> This is why it requires massive data; it must learn the manifold’s topology from scratch, unlike CNNs or RNNs which possess inherent locality priors.</p> <h2 id="ii-scaling-law-and-emergence"><strong>II. Scaling Law and Emergence</strong></h2> <p>Architecturally, LLMs (like GPT-4, LLaMA) differ little from the original Transformer; the primary difference is scale. Under the Manifold Hypothesis, the shift from small to large models is not merely quantitative accumulation but a qualitative change in manifold topology, coverage density, and connectivity.</p> <h3 id="1-scaling-law"><strong>1. Scaling Law</strong></h3> <p>In deep learning, Loss essentially measures the distance between the manifold learned by the model and the true data manifold. The Scaling Law describes the power-law decrease of Loss with respect to compute \(C\), parameters \(N\), and data \(D\): \(L(N) \propto N^{-\alpha}\).</p> <ul> <li> <p><strong>Increasing Parameters \(N\) (Reducing Bias):</strong> The Scaling Law suggests that as \(N\) increases, the model’s ability to fit high-frequency curvature rises according to a power law. Small models can only learn the global skeleton of the manifold, i.e., the principal components. At this stage, Loss drops rapidly. As \(N\) increases, the model begins to wrap around high-frequency regions with extreme curvature on the manifold—the rare, complex long-tail samples.</p> </li> <li> <p><strong>Increasing Data \(D\)(Reducing Variance):</strong> According to coverage number theory, covering a \(d\)-dimensional manifold with precision \(\epsilon\) requires sample size\(M \propto (1/\epsilon)^d\). The Scaling Law effectively reveals the decay rate of approximation error as sample density increases for a specific \(d\). This explains why image generation (high \(d\)) is harder to scale than text classification (low \(d\)). The existence of the Scaling Law proves that deep networks are indeed performing manifold learning, not simple memorization. If it were memorization, the Loss curve would not exhibit this power-law distribution.</p> </li> </ul> <p>From the Foundation Model perspective, the training of an LLM is the ultimate approximation of the language manifold.</p> <h3 id="2-emergence-phase-transition-on-the-manifold"><strong>2. Emergence: Phase Transition on the Manifold</strong></h3> <p>If we view data distribution as a manifold, learning is the process of establishing connectivity upon it.</p> <ul> <li> <p><strong>Small Models</strong>: The model learns dispersed local neighborhoods on the manifold but fails to establish correct mappings between them. The model cannot perform multi-step reasoning because the inference path is broken.</p> </li> <li> <p><strong>Critical Point</strong>: When parameter count \(N\) and training data \(D\) exceed a certain threshold, the model’s coverage density on the manifold reaches the percolation threshold. Dispersed local knowledge suddenly connects into a globally consistent graph.</p> </li> <li> <p><strong>Emergence</strong>: At this point, the model can not only interpolate but also perform transitive composition on the manifold. For instance, knowing A→B and B→C leads to the emergence of A→C capability. Macroscopically, this manifests as a sudden jump in performance (similar to phase transitions in complex physical systems).</p> </li> </ul> <p>Emergence is often accompanied by geometric reconstruction (linearization and disentanglement) of the representation space. Before the phase transition, different concepts are entangled and twisted on the manifold, inseparable by linear layers. After the transition, the model learns to unfold the curved manifold into a high-dimensional Euclidean space, making complex semantic relations linearly separable. Acquiring this unfolding capability often requires achieving certain depth and width—precisely the moment emergence occurs.</p> <p><strong>Why can’t other architectures (CNN/RNN) achieve the same degree of emergence?</strong></p> <p>The root cause remains <strong>Inductive Bias.</strong></p> <p>RNN forces all historical information into a fixed-dimension state vector \(h_t\). For complex manifold trajectories, this equates to projecting a high-dimensional manifold into a low-dimensional space, inevitably leading to <strong>information loss and Singularities</strong>. As sequences lengthen and trajectories diverge, the model cannot maintain the global geometric structure of the manifold with finite memory, thus failing to emerge long-range reasoning capabilities. While CNN is efficient, its receptive field grows linearly. Covering two distant associated points on the manifold requires stacking extremely deep networks, leading to optimization difficulties. More importantly, CNN’s weight sharing assumes identical geometric properties across the manifold, limiting its ability to handle non-stationary semantic manifolds.</p> <p>Furthermore, the Transformer’s Attention is the product of input and weights, or even the quadratic form of the input itself ($Q K^T$). This makes it essentially a second-order (or higher) network, whereas other architectures are mostly first-order accumulations. High-order interactions allow the Transformer to dynamically adjust computation weights based on context. This in-context learning capability is difficult for fixed-weight RNN/CNNs to possess.</p> <h2 id="iii-efficient-modeling-of-manifolds"><strong>III. Efficient Modeling of Manifolds</strong></h2> <p>Even by weakening inductive bias, the Transformer gains context-adaptive modeling capabilities. However, this adaptation currently manifests as adjustments for input/output tokens within a unified data manifold, rather than model-level adaptation for different data distributions/manifolds. If we had infinite labor, data, and compute, we could train a specific small-scale Transformer for every sub-task instead of brute-force fitting a billion-parameter large model. Many current architectural optimizations for Transformers are implicitly doing this. For example, MoE partitions regions in FFN memory, while Sparse/Gated Attention establishes highly flexible and sparse connections between coordinates on the manifold.</p> <h3 id="1-moe"><strong>1. MoE</strong></h3> <p>The essence of MoE is acknowledging that using a single, globally shared Dense Model to approximate a high-dimensional manifold with extremely complex topological structure and drastic curvature changes is mathematically inefficient and unstable. If the real data distribution consists of multiple sub-manifolds, a Dense Model learns a single operator under a global unified coordinate system (analogous to a second-order MLP); an MoE model learns multiple local operators (covering different sub-manifold regions) plus a router (learning which region an input token belongs to and which local operators to invoke).</p> <p><strong>Why is MoE suitable for the Long Tail and Multi-Domain aspect of Foundation Models?</strong></p> <p><strong>Long-tail samples</strong> often fall into regions rarely visited by the Dense Model, equivalent to sparse branches or small fragments of the sub-manifold. Simultaneously, a Dense Model with a fixed compute budget struggles to maintain high-resolution approximation across all regions. MoE allocates resolution on demand: letting specific experts handle specific regions, thereby improving coverage of multi-domain structures under the same token compute budget.</p> <h3 id="2-sparse-attention"><strong>2. Sparse Attention</strong></h3> <p>Traditional Attention builds a graph and performs diffusion/kernel smoothing on the token manifold. If token representations lie on a low-dimensional manifold, the most effective information often comes from manifold neighbors or a few cross-region shortcuts. Fully connected attention introduces massive non-manifold noise connections (long-distance, semantically unrelated token interactions); thus, effective mixing does not require full connectivity. The core of Sparse Attention is approximating the full connection in traditional Attention as a sparse graph:</p> <ul> <li> <p><strong>Top-k / kNN Sparsification</strong>: Retain only the \(k\) edges with maximum similarity for each query (manifold neighbors).</p> </li> <li> <p><strong>Block Sparse / Local Window</strong>: Prior assumption that neighbors are also local in sequence position (suitable for local dependencies).</p> </li> <li> <p><strong>Budget Adaptive Sparsity</strong>: Different sparsity levels for different tokens and different stages (prefill vs decode).</p> </li> </ul> <h3 id="3-gated-attention"><strong>3. Gated Attention</strong></h3> <p>The recently emerging Gated Attention goes beyond fine-tuning the attention matrix (which often introduces new inductive biases) to directly adaptively controlling the projected vector field. The Gated mechanism introduces a gating signal \(g \in [0,1]^d\) to filter the update vector dimension-wise. Geometrically, this is orthogonal decomposition and suppression of tangent vectors. The Gate identifies and suppresses noise components perpendicular to the current task sub-manifold, retaining only effective components along the geodesic direction. Furthermore, the gating mechanism is most effective when applied to the Q vector, stemming from the rank elevation of the projection matrix brought by sparsity.</p> <p>When processing long sequences, Gated Attention dynamically adjusts the decay rate of historical information and avoids Attention Sinks. This is equivalent to dynamically adjusting inertia based on the manifold’s local curvature. In flat regions of the manifold (semantic coherence), the Gate allows long-distance information transmission; in regions with curvature mutations (semantic shifts), the Gate rapidly truncates historical dependencies and resets the trajectory direction.</p> <p>These architectural optimizations transcend stabilizing the training process or enhancing feature representation itself. Deeper reasoning often corresponds to <strong>more abstract embeddings and increased information density of single embeddings</strong>. MoE and Sparse/Gated Attention provide the structural foundation for such high signal-to-noise ratio representations. Supported by training data (higher quality) and training methods (deeper RL), the chain-of-thought/graph built by such sparse routing-like mechanisms will demonstrate greater potential. Understood at this level, <strong>the sparsity of thought reflects the model’s level of intelligence to some extent</strong>.</p> <h2 id="iv-true-and-false-reasoning"><strong>IV. True and False Reasoning</strong></h2> <h3 id="1-in-context-learning-icl"><strong>1. In-Context Learning (ICL)</strong></h3> <p>Current ICL in LLMs is essentially a mathematical abstraction of dynamic localization and interpolation of high-dimensional data within a low-dimensional topological structure. ICL is not “learning” new knowledge but retrieving, locating, and locally linearizing the low-dimensional manifold structures constructed during pre-training.</p> <p>In the inference phase, the Transformer uses examples in the Prompt (acting geometrically as anchors on the manifold) to dynamically locate a specific task sub-manifold in the latent space. The Attention mechanism effectively calculates the projection of the input Query onto the tangent space of this sub-manifold. The model merely locks onto a local coordinate system within the vast, learned manifold structure via the Prompt.</p> <p>The forward inference process of the Transformer is mathematically equivalent to gradient descent updates on a loss function defined by context examples, performed in parameter space or activation space. Given context, the model does not update physical weights \(W\) but simulates the process of finding the optimal solution on \(M_{task}\) by changing internal activation states. This makes ICL appear as local fine-tuning within specific regions of the manifold.</p> <p>However, ICL cannot perform true reasoning or create new knowledge. It can only interpolate within the convex hull or neighborhood of the pre-training manifold. If the logic required for the task is completely orthogonal to the pre-training manifold (i.e., Out-of-Distribution), ICL is bound to fail or hallucinate. Furthermore, as context length increases, conflicting or noisy examples can cause unstable localization of activation points on the manifold, or even collapse into incorrect sub-manifold regions.</p> <h3 id="2-cot-path-planning-with-explicit-symbols"><strong>2. CoT: Path Planning with Explicit Symbols</strong></h3> <p>From the rigorous perspective of the Manifold Hypothesis, Chain of Thought is not the emergence of reasoning, but a geometric strategy to reduce mapping curvature and perform geodesic interpolation.</p> <p>In existing LLMs, for simple tasks, input \(x\)and output\(y\) are close on the manifold, or the manifold is locally flat. In this case, single-step reasoning \(y=f(x)\) is effective because it can be approximated by simple linear interpolation or shallow non-linear transformation. However, for complex tasks, although \(x\) and \(y\) are both on the manifold, the geodesic distance between them is immense, and the manifold structure is highly curled, non-convex, or even contains topological holes. If we force the model to directly predict \(P(y \mid x)\), the model attempts to walk a straight line connecting \(x\) and \(y\) in the ambient space. But this straight line often cuts through OOD regions outside the manifold, leading to hallucinations or logical breaks.</p> <p>CoT forces the model to generate intermediate steps \(z_1, z_2, ..., z_n\) between \(x\) and \(y\). This effectively decomposes an extremely difficult global mapping problem into a series of local mapping problems with low Lipschitz constants, explicitly eliminating uncertainty. The token sequence generated by CoT is a discretized geodesic path on the manifold surface.</p> <h3 id="3-latent-cot-returning-to-continuous-manifold-dynamics"><strong>3. Latent CoT: Returning to Continuous Manifold Dynamics</strong></h3> <p>However, human language is a low-dimensional, quantized projection of the thought manifold. Forcing a model to output natural language for reasoning is equivalent to forcing continuous neural network signals through a low-bandwidth Argmax discretization layer. Simultaneously, many intuitive, fuzzy intermediate states cannot be precisely captured by discrete vocabulary. <strong>This quantization noise accumulates in long-chain reasoning, leading to reasoning drift.</strong></p> <p>Latent CoT attempts to remove the constraint of discrete Tokens, evolving trajectories directly in Latent Space. In this case, within Latent Space, the model can maintain a superposition state of being both A and B until the final moment of inference, when it collapses into a discrete output. This avoids error propagation caused by premature discretization decisions. Additionally, a single Token can only transmit \(log_2∥V∥\) bits of information (where \(∥V∥\) is vocabulary size), whereas a \(d\)-dimensional FP16 vector can theoretically transmit far more information. Latent CoT exploits this high-dimensional broadband channel to transmit complex reasoning states.</p> <p>This internal thinking essentially increases the depth of the computational graph. From a manifold perspective, this involves progressively stretching and untangling the originally entangled manifold through multiple composite function transformations until it becomes linearly separable.</p> <p>For Latent CoT, since there is no explicit Ground Truth text, training methods are shifting from SFT to RL. The objective function is no longer predicting the next Token but maximizing the accuracy of the final answer. The model is encouraged to freely explore paths in the latent space. Research finds that the Latent Thoughts learned by the model often exhibit feature distributions incomprehensible to humans—precisely proving that it has broken through the low-dimensional manifold limits of human language and found superior Shortcuts.</p> <p><strong>But is reasoning based on Chain of Thought general reasoning?</strong></p> <p>The reasoning demonstrated by current LLMs is mathematically primarily <strong>local interpolation</strong> on high-dimensional manifolds. The essence of the Scaling Law is that as parameters increase, the sampling density of the manifold rises exponentially, causing the probability of a test sample falling into the \(\epsilon\)-neighborhood of a training sample to approach 1. Human thinking, however, possesses the capability of extrapolation off the manifold. Humans can establish new logical connections in the holes of the data manifold or in directions completely orthogonal to the manifold.</p> <p>Additionally, human language is a <strong>low-dimensional, lossy, highly compressed projection</strong> of the physical world. Humans rarely expand their understanding of the real world solely using language; even theorems and conjectures derived from mathematics/physics require experimental verification in the real world to be widely accepted. LLMs attempt to reconstruct \(M_{world}\) via \(M_{language}\), i.e., building a world model. But since this projection is not bijective, reverse-engineering the world from language has infinite solutions. Therefore, interaction with the environment remains indispensable.</p> <p>Finally, existing large models, even if adaptive, remain static. When data distribution (manifold structure) changes, without retraining or fine-tuning, they will still hit the OOD barrier. Catastrophic forgetting caused by retraining and fine-tuning remains a hurdle difficult to overcome (hence the recent surge in continual learning). Without dynamics, humanity is destined to burn astronomical amounts of capital and electricity in the repeated training of models.</p> <h2 id="epilogue"><strong>Epilogue</strong></h2> <p>I have written too much to ramble further.</p> <p>In short, amidst the waves of chasing hotspots, do not forget that gold often already exists in the shadows of the path traveled.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/24/spacedrive.html">SpaceDrive：为自动驾驶VLA注入空间智能</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/24/spacedrive.html">SpaceDrive：Infusing Spatial Awareness into VLM-based Autonomous Driving</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/15/manifold-hypothesis-2.html">流形假设(Manifold Hypothesis)下的思考·二</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/11/25/manifold-hypothesis-1.html">流形假设(Manifold Hypothesis)下的思考·一</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/11/25/manifold-hypothesis-1.html">In the Perspective of Manifold Hypotheses - 1</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Peizheng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>