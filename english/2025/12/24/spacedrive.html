<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SpaceDrive：Infusing Spatial Awareness into VLM-based Autonomous Driving | Peizheng Li </title> <meta name="author" content="Peizheng Li"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?343676cba58161999546fb5ac8b9e53d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://edwardleelpz.github.io/english/2025/12/24/spacedrive.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Peizheng</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SpaceDrive：Infusing Spatial Awareness into VLM-based Autonomous Driving</h1> <p class="post-meta"> Created on December 24, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> Paper   ·   <i class="fa-solid fa-tag fa-sm"></i> English </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/spacedrive_teaser-480.webp 480w,/assets/img/blog_images/spacedrive/spacedrive_teaser-800.webp 800w,/assets/img/blog_images/spacedrive/spacedrive_teaser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/spacedrive_teaser.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Paper Title:</strong> SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving</p> <p><strong>Paper Link:</strong> <a href="https://arxiv.org/abs/2512.10719" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2512.10719</a></p> <p><strong>Project Page:</strong> <a href="https://zhenghao2519.github.io/SpaceDrive_Page/" rel="external nofollow noopener" target="_blank">https://zhenghao2519.github.io/SpaceDrive_Page/</a></p> <p><strong>Authors:</strong> <a href="https://edwardleelpz.github.io/" rel="external nofollow noopener" target="_blank"><em>Peizheng Li</em></a>, <a href="https://zhenghao2519.github.io/" rel="external nofollow noopener" target="_blank"><em>Zhenghao Zhang</em></a>, <a href="https://scholar.google.com/citations?user=gf09DbwAAAAJ&amp;hl=en&amp;oi=sra" rel="external nofollow noopener" target="_blank"><em>David Holtz</em></a>, <a href="https://scholar.google.com/citations?user=yEY9n1EAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><em>Hang Yu</em></a>, <a href="https://scholar.google.com/citations?user=kg9OvU0AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><em>Yutong Yang</em></a>, <a href="https://scholar.google.com/citations?user=9Z6Gjo4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><em>Yuzhi Lai</em></a>, <a href="https://rruisong.github.io/" rel="external nofollow noopener" target="_blank"><em>Rui Song</em></a>, <a href="https://www.cvlibs.net/" rel="external nofollow noopener" target="_blank"><em>Andreas Geiger</em></a>, <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/kognitive-systeme/the-chair/staff/prof-dr-andreas-zell/" rel="external nofollow noopener" target="_blank"><em>Andreas Zell</em></a></p> <p><strong>Affiliations:</strong> Mercedes-Benz AG, University of Tübingen, Tübingen AI Center, TU Munich, Karlsruhe Institute of Technology, University of Stuttgart, UCLA</p> <h2 id="abstract"><strong>Abstract</strong></h2> <p>Vision-Language-Action Models (VLAs) are emerging as a new paradigm for end-to-end autonomous driving, leveraging their strong generalization and semantic understanding capabilities. However, existing 2D VLM-based driving systems exhibit significant shortcomings in handling fine-grained 3D spatial relationships, which are core requirements for spatial reasoning and trajectory planning. To address this, Mercedes-Benz and the University of Tübingen jointly propose <strong>SpaceDrive</strong>, a spatially-aware VLM-based autonomous driving framework. Its core innovation lies in abandoning the conventional VLM approach of treating coordinate values as text tokens, and instead introducing <strong>3D Positional Encoding (PE) as a universal spatial representation</strong>. Specifically, SpaceDrive first explicitly fuses visual tokens with 3D PE in the feature space. It then uses the same universal 3D PE to replace the corresponding coordinate text tokens in the prompt, serving as the interface for the foundation model’s input and output. Furthermore, SpaceDrive employs a regression decoder instead of a classification head to predict planned trajectory coordinates, circumventing the inherent limitations of language models in numerical processing. Experiments show that compared to existing VLM/VLA methods, SpaceDrive achieves state-of-the-art (SOTA) performance in open-loop evaluation on nuScenes and ranks second in the closed-loop Bench2Drive evaluation with a driving score of 78.02, significantly improving planning geometric accuracy and safety.</p> <h2 id="core-insights"><strong>Core Insights</strong></h2> <p>Current VLM applications in autonomous driving face two fundamental limitations that constrain their potential as general driving agents:</p> <ul> <li> <strong>Disconnect between 2D Semantics and 3D Geometry:</strong> VLMs are primarily pre-trained on large-scale 2D image-text pairs, leading to a severe lack of 3D spatial priors. This results in ambiguous scene descriptions and defective spatial reasoning capabilities.</li> <li> <strong>Defects of Numerical Tokenization:</strong> In language models, coordinates are typically decomposed digit-by-digit into characters or numbers (e.g., “3.82” becomes “3”, “.”, “8”, “2”). This process essentially fits the joint distribution of tokens rather than performing numerical computation. It ignores the continuous, neighboring structure of numerical values (e.g., “3.72” is closer to “3.82” than “3.12”) and averages the importance of tokens from different digit positions (e.g., equal loss weight for “3” and “2” in “3.82”), fundamentally limiting the accuracy and stability of continuous numerical prediction.</li> </ul> <p>Existing VLM-based planners often overlook these issues or resort to training specific embeddings/queries for particular tasks to predict coordinates, making them difficult to transfer to upstream reasoning or other tasks.</p> <p>However, <strong>the Positional Encoding (PE) within the Transformer architecture inherently handles positional relationships between tokens</strong>, which can be viewed as <strong>spatial relationships between semantic features</strong>. Inspired by this, SpaceDrive replaces textual numerical tokens with an <strong>explicit, unified 3D Positional Encoding</strong>. This converts coordinate descriptions into a unified representation that is computable, alignable, and directly usable by attention mechanisms, thereby enhancing the system’s spatial reasoning and trajectory planning capabilities.</p> <h2 id="method"><strong>Method</strong></h2> <p>The core of the SpaceDrive framework is its unified spatial interface:</p> <ul> <li> <strong>Visual Side:</strong> A frozen depth estimator obtains absolute depth for each image patch, which is projected to 3D coordinates. These coordinates are then encoded by a PE module and added to the corresponding visual tokens, yielding spatial-aware visual tokens.</li> <li> <strong>Text Side:</strong> After tokenization, the text is scanned for coordinate expressions. Their numerical values are parsed and encoded by the same PE encoder to produce spatial tokens, which replace the original sequence of number tokens. A special prefix indicator ⟨IND⟩ marks these tokens.</li> <li> <strong>Output Side:</strong> The language head generates text normally. When ⟨IND⟩ is generated, the subsequent hidden state is fed into a PE decoder to directly regress 3D/BEV coordinates, replacing the digit-by-digit generation of numbers.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/spacedrive_architecture-480.webp 480w,/assets/img/blog_images/spacedrive/spacedrive_architecture-800.webp 800w,/assets/img/blog_images/spacedrive/spacedrive_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/spacedrive_architecture.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">SpaceDrive Architecture</figcaption> </figure> </div> </div> <h3 id="perception-phase-explicit-fusion-of-vision-and-depth">Perception Phase: Explicit Fusion of Vision and Depth</h3> <p>While using a VLM’s pre-trained visual encoder to extract visual tokens, SpaceDrive employs a frozen depth estimator (e.g., UniDepthV2) to obtain absolute depth. Combining this with camera intrinsics and extrinsics, the center of each image patch is projected into metric 3D space \(\mathbf{c}_p = (x_p^{3D}, y_p^{3D}, z_p^{3D})\). These 3D coordinates are mapped to a PE vector \(\phi(\mathbf{c}_p)\) of the same dimension as the tokens by a universal PE encoder. To avoid confusion with the base VLM’s RoPE, SpaceDrive adopts Sine-cosine encoding as the PE encoder:</p> \[\phi(\mathbf{c}_p)=\big[\phi_x(x_p^{3D}),\phi_y(y_p^{3D}),\phi_z(z_p^{3D})\big]\in\mathbb{R}^{dim}, \text{with} \\\] \[\phi_a(p_a) =\begin{cases} \sin(\tfrac{p_a}{20000^{2i/d_a}}),\\ \cos(\tfrac{p_a}{20000^{2i/d_a}}), \end{cases} i=0,\dots,\lfloor\tfrac{d_a}{2}\rfloor-1,\\\] \[d_x=d_y=\lceil\tfrac{dim}{3}\rceil, d_z={dim}-d_x-d_y.\] <p>The channels of the above 3D PE are allocated along the \(x/y/z\) dimensions. This 3D PE is then directly added to the modality-aligned visual token \(h_p\), thereby infusing the VLM’s visual input with absolute spatial coordinate information:</p> \[\tilde{h}_p = h_p + \alpha_{PE}\, \phi(\mathbf{c}_p).\] <p>Considering that sparse queries (like in Q-Former) are difficult to densely align with specific 3D locations and require additional alignment pre-training, visual tokens in SpaceDrive are aligned with the language space via an MLP projector. The \(\alpha_{PE}\) in the formula is a learnable normalization factor to avoid training instability caused by deviation of the token norm distribution from the pre-training distribution.</p> <p><strong>Spatial Information Retrieval:</strong> Since attention is based on dot-product similarity retrieval, adding 3D PE to visual tokens essentially makes spatial location a <strong>key-value structure directly retrievable by attention</strong>. Consequently, coordinate PEs in subsequent text can use similarity to index semantic features at corresponding spatial locations, rather than relying on the language model to guess.</p> <h3 id="reasoning-phase-unified-coordinate-interface">Reasoning Phase: Unified Coordinate Interface</h3> <p>When 3D coordinates appear in the input prompt, for a coordinate substring \(S_r\) in the text prompt, its numerical value \(\mathbf{c}_r = (x_r,y_r,z_r)\) is extracted and encoded using the same unified PE encoder \(\phi(\cdot)\). These encoded 3D PEs replace the original sequence of number tokens and are preceded by the special token ⟨IND⟩ to avoid semantic confusion (for special cases like BEV coordinates, e.g., trajectory waypoints, the \(z\)-axis component in the PE is set to 0 to avoid affecting attention calculation).</p> \[\tilde{h}_i=\begin{cases}\phi(\mathbf{c}_r) &amp; i\in\mathcal{S}_r \\ \mathrm{Tokenizer}(t_i) &amp; \text{otherwise}\end{cases} .\] <p>In addition to basic prompt input, the vehicle’s Ego Status has proven highly effective for trajectory planning. Existing methods typically encode all state variables (e.g., pose, velocity, acceleration) into a single vector embedding \(\mathbf{e}_{\text{ego}}\in\mathbb{R}^{dim}\). Benefiting from the unified spatial representation, SpaceDrive can also encode historical Ego waypoints using the same \(\phi(\cdot)\) and input them along with \(\mathbf{e}_{\text{ego}}\) as explicit spatiotemporal conditions to the language model for precise trajectory planning.</p> <p><strong>Logical Consistency:</strong> By using the same set of PE for vision, text prompts, and Ego waypoints, the model is compelled to learn a unified spatial semantic indexing, rather than learning disjointed mappings for different modalities.</p> <h3 id="output-phase-regression-over-classification">Output Phase: Regression Over Classification</h3> <p>During output generation, when the model’s language head predicts the special indicator token ⟨IND⟩ from \(\mathbf{e}_j\), the next step’s embedding output \(\mathbf{e}_{j+1}\) will be decoded into 3D coordinates by a dedicated PE decoder \(\psi(\cdot)\):</p> \[\hat{\mathbf{c}} = \psi(\mathbf{e}_{j+1}), \, \hat{\mathbf{c}} \in \mathbb{R}^3.\] <p>Considering that Sine-cosine PE is not analytically invertible (phase/frequency aliasing), the PE decoder is made learnable. This decoder can employ an MLP for deterministic coordinate regression output or choose a generative module like a VAE for multi-modal output. SpaceDrive defaults to using a lightweight MLP as the PE decoder.</p> <h3 id="loss-function"><strong>Loss Function</strong></h3> <p>For coordinate prediction, SpaceDrive employs Huber Loss for supervision, which balances outliers and convergence accuracy better than L2 or L1 loss. The text part retains the original cross-entropy loss:</p> \[\mathcal{L} = \mathcal{L}_{\text{LM}} + \mathcal{L}_{\text{Huber}}(\hat{\mathbf{c}}, \mathbf{c}).\] <h2 id="experiments-and-visualization"><strong>Experiments and Visualization</strong></h2> <p>The paper validates SpaceDrive and SpaceDrive+ (with Ego Status input) for open-loop and closed-loop planning on the nuScenes dataset and the Bench2Drive benchmark, respectively. In experiments, the framework is based on the Qwen2.5-VL-7B VLM, fine-tuned using LoRA alignment with a rank of 16. A frozen pre-trained Unidepthv2-ViT-L serves as the depth estimation module. For open-loop planning, the model predicts 6 points over a 3-second future horizon. Closed-loop planning follows SimLingo, outputting both path and speed waypoints for vehicle PID control.</p> <h3 id="open-loop-planning-nuscenes"><strong>Open-loop Planning (nuScenes)</strong></h3> <p>To directly verify trajectory planning accuracy, an open-loop evaluation was first conducted. On the nuScenes dataset, SpaceDrive+ outperforms existing VLM-based methods like OmniDrive/ORION across all metrics (Avg. L2 = 0.32m, Avg. Collision = 0.23%, Avg. Intersection = 1.27%).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/openloop_eval-480.webp 480w,/assets/img/blog_images/spacedrive/openloop_eval-800.webp 800w,/assets/img/blog_images/spacedrive/openloop_eval-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/openloop_eval.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">开环规划性能对比</figcaption> </figure> </div> </div> <p>Notably: The SpaceDrive framework <strong>does not rely on BEV features at all</strong>. The results still demonstrate that the unified positional encoding interface is sufficient to support 3D spatial modeling within the VLM, architecturally reducing dependency on dense BEV representations.</p> <h3 id="closed-loop-planning-bench2drive"><strong>Closed-loop Planning (Bench2Drive)</strong></h3> <p>Considering that similarity-based open-loop planning evaluation is highly susceptible to dataset overfitting and cannot fully reflect a model’s actual driving capability, the paper further validates the method’s effectiveness in the closed-loop Bench2Drive benchmark.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/closeloop_eval-480.webp 480w,/assets/img/blog_images/spacedrive/closeloop_eval-800.webp 800w,/assets/img/blog_images/spacedrive/closeloop_eval-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/closeloop_eval.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">闭环规划性能对比</figcaption> </figure> </div> </div> <p>The paper first attempted a method using only text generation for trajectories. Experiments proved this method tends to degenerate into near-linear trajectories with oscillating headings in closed-loop, being highly unstable. This is because text generation essentially fits data priors rather than learning a controllable policy. In contrast, after introducing explicit universal spatial tokens, SpaceDrive+ achieves a Driving Score of 78.02 and a Success Rate of 55.11%, ranking second among VLM-based methods.</p> <h3 id="visualization"><strong>Visualization</strong></h3> <p>The paper compares the performance of the pure text method and the method incorporating spatial tokens in the same scenario (lane change to avoid a cyclist):</p> <ul> <li>The pure text method’s output trajectory degenerates into a straight line with constantly oscillating direction, eventually causing the vehicle to swerve left significantly until colliding with a guardrail.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/closeloop_vis_omnidrive-480.webp 480w,/assets/img/blog_images/spacedrive/closeloop_vis_omnidrive-800.webp 800w,/assets/img/blog_images/spacedrive/closeloop_vis_omnidrive-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/closeloop_vis_omnidrive.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Pure text method degenerates in closed-loop simulation</figcaption> </figure> </div> </div> <ul> <li>SpaceDrive+ with spatial tokens, upon observing a slow cyclist ahead, first tentatively accelerates to seek an overtaking opportunity. Finding the adjacent vehicle does not yield, it decelerates to create a safe insertion gap, then decisively changes lanes, and corrects the steering in time before completing the lane change to avoid leaving the road.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/closeloop_vis_spacedrive-480.webp 480w,/assets/img/blog_images/spacedrive/closeloop_vis_spacedrive-800.webp 800w,/assets/img/blog_images/spacedrive/closeloop_vis_spacedrive-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/closeloop_vis_spacedrive.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">SpaceDrive can handle complex closed-loop driving scenarios</figcaption> </figure> </div> </div> <h2 id="ablation-studies"><strong>Ablation Studies</strong></h2> <p>To further validate how the universal 3D PE contributes to planning, numerous ablation studies were conducted, leading to the following conclusions:</p> <ul> <li> <strong>PE Injection Location is Crucial:</strong> Using PE only for text coordinate replacement without injecting it into visual tokens offers limited improvement (as PE cannot index corresponding visual features). Injecting 3D PE into visual tokens brings significant gains. When unified positional encoding is applied to both visual and text coordinate streams, planning performance improves regardless of ego state usage, highlighting the value of a shared spatial representation.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/abl_pe-480.webp 480w,/assets/img/blog_images/spacedrive/abl_pe-800.webp 800w,/assets/img/blog_images/spacedrive/abl_pe-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/abl_pe.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Ablation Study on PE Injection Location</figcaption> </figure> </div> </div> <ul> <li> <strong>Choice of PE Encoder/Decoder is Important:</strong> Sine-cosine encoding inherently offers better translation equivariance, aiding the attention mechanism in understanding spatial relationships between tokens, outperforming a learnable MLP encoder. RoPE conflicts with the base VLM’s RoPE, causing semantic instability in outputs. Directly inverting sine-cosine at the output is ill-posed, and the VLM’s output space is not fully aligned with its input embedding space. Therefore, a learnable, per-waypoint MLP decoder is superior.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/abl_pe_en_de-480.webp 480w,/assets/img/blog_images/spacedrive/abl_pe_en_de-800.webp 800w,/assets/img/blog_images/spacedrive/abl_pe_en_de-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/abl_pe_en_de.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Ablation Study on PE Encoder and Decoder</figcaption> </figure> </div> </div> <ul> <li> <strong>Learnable \(α_{PE}\) is Important:</strong> Fixed-scale PE easily causes semantic instability or convergence difficulties, while a learnable α_{PE} significantly improves L2 error, collision rate, and out-of-bounds rate.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/abl_pe_norm-480.webp 480w,/assets/img/blog_images/spacedrive/abl_pe_norm-800.webp 800w,/assets/img/blog_images/spacedrive/abl_pe_norm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/abl_pe_norm.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Ablation Study on PE Norm</figcaption> </figure> </div> </div> <ul> <li> <strong>PE Representation as an Interface is Transferable:</strong> The same set of PE spatial interface yields similar performance gains on both Qwen-VL and LLaVA, indicating that the benefits stem primarily from the unified spatial reasoning interface rather than specific adaptation to a particular base model.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/spacedrive/abl_vlm-480.webp 480w,/assets/img/blog_images/spacedrive/abl_vlm-800.webp 800w,/assets/img/blog_images/spacedrive/abl_vlm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/spacedrive/abl_vlm.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Ablation Study on Different VLM Base Models</figcaption> </figure> </div> </div> <p>Additionally, supplementary materials present more experiments related to VQA tasks, different depth estimation models, and various hyperparameters, further confirming the effectiveness of the proposed method.</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>SpaceDrive makes several key contributions to current autonomous driving and VLM research:</p> <ul> <li> <strong>Universal Spatial Representation:</strong> It introduces a unified 3D Positional Encoding that works consistently across perception, reasoning, and planning modules, representing a significant architectural innovation. This approach moves beyond task-specific embeddings towards more general spatial intelligence.</li> <li> <strong>Explicit 3D Understanding:</strong> Additively integrating spatial encoding with visual tokens creates an explicit link between semantic content and 3D location, enabling more accurate scene understanding and reasoning.</li> <li> <strong>Regression Respects Numerical Nature:</strong> By replacing digit-by-digit coordinate generation with regression-based dedicated decoding, SpaceDrive addresses a fundamental limitation of language models in handling continuous numerical quantities.</li> <li> <strong>Framework Generality:</strong> The method demonstrates compatibility with different VLM architectures (Qwen-VL, LLaVA) and proves suitable for inference-time enhancements like chain-of-thought reasoning, indicating broad applicability.</li> </ul> <p>In summary, SpaceDrive provides a rigorous paradigm shift: <strong>from “modeling geometry with language” to “explicitly encoding geometry.”</strong> Its core contribution lies in demonstrating that within VLMs, a <strong>unified, modality/task-agnostic 3D positional encoding</strong> can effectively connect the perceived visual space with the planned physical space. This approach not only addresses the hallucination and accuracy issues of VLMs in large-scale spatial reasoning tasks but also preserves their general advantage in long-tail scene understanding. SpaceDrive represents a significant step towards enabling VLMs to interact effectively with the physical world through precise spatial understanding, pointing the way forward for more reliable and capable AI agents.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/24/spacedrive.html">SpaceDrive：为自动驾驶VLA注入空间智能</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/12/15/manifold-hypothesis-2.html">流形假设(Manifold Hypothesis)下的思考·二</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/12/15/manifold-hypothesis-2.html">In the Perspective of Manifold Hypotheses - 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/%E4%B8%AD%E6%96%87/2025/11/25/manifold-hypothesis-1.html">流形假设(Manifold Hypothesis)下的思考·一</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/english/2025/11/25/manifold-hypothesis-1.html">Reflections on the Manifold Hypothesis - 1</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Peizheng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>